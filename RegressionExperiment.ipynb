{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup code\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16.0, 16.0) # set default size of plots\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Housing Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (404, 13)\n",
      "Train labels shape:  (404,)\n",
      "Validation data shape:  (102, 13)\n",
      "Validation labels shape:  (102,)\n"
     ]
    }
   ],
   "source": [
    "# Load the raw housing_scale data.\n",
    "filename = 'dataset/housing_scale'\n",
    "X, y = load_svmlight_file(filename)\n",
    "\n",
    "# Split the data into train, and val sets.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.91702508 -0.76945545 -0.22159092 -0.84158416 -0.308534    0.03766704\n",
      "   0.33956011 -0.50958712 -0.26345231 -0.17459005  0.24225822  0.79955463\n",
      "  -0.3902572 ]]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing: subtract the mean of each housing feature\n",
    "# first: compute the housing feature mean based on the training data\n",
    "\n",
    "_mean = np.mean(X_train, axis=0)\n",
    "print(_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second: subtract the mean from train and X_val data\n",
    "X_train -= _mean\n",
    "X_val -= _mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 14) (102, 14)\n"
     ]
    }
   ],
   "source": [
    "# third: append the bias dimension of ones\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels shape:  (404, 1)\n",
      "Validation labels shape:  (102, 1)\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_val = y_val.reshape(-1, 1)\n",
    "\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation labels shape: ', y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regresser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regresser_loss_naive(w, X_train, y_train, reg) return loss, grad\n",
    "\n",
    "regresser_loss_vectorized(w, X_train, y_train, reg) return loss, grad\n",
    "\n",
    "prediction = \n",
    "loss = 0.5 * (_y - y)**2 + reg * sum(W**2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regresser_loss_naive(W, X, y, reg):\n",
    "    '''\n",
    "    linear regression loss function, naive implementation (with loops).\n",
    "    \n",
    "    Inputs:\n",
    "    - W: A numpy array of shape (D, 1) containing weights.\n",
    "    - X: A numpy array of shape (N, D) containing a minibatch of data.\n",
    "    - y: A numpy array of shape (N, 1) containing training labels.\n",
    "    - reg: (float) regularization strength\n",
    "    \n",
    "    Returns a tuple of:\n",
    "    - loss as single float\n",
    "    - gradient with respect to weights W; an array of same shape as W\n",
    "    '''\n",
    "    num_train = X.shape[0]\n",
    "    \n",
    "    loss = 0.0\n",
    "    grad = np.zeros(W.shape) # initialize the gradient as zero\n",
    "    \n",
    "    # compute the loss and the gradient\n",
    "    for i in range(num_train):\n",
    "        _y = np.sum(X[i] * W)\n",
    "        loss += (y[i][0] - _y)**2\n",
    "        grad += - 2 * X[i].T * (y[i][0] - _y)\n",
    "    loss /= num_train\n",
    "    grad /= num_train\n",
    "    \n",
    "    # Add regularization to the loss and gradient.\n",
    "    loss += reg * 0.5 * np.sum(np.square(W))\n",
    "    grad += reg * W\n",
    "    \n",
    "    return loss, grad\n",
    "\n",
    "def regresser_loss_vectorized(W, X, y, reg):\n",
    "    '''\n",
    "    linear regression loss function, vectorized implementation.\n",
    "    \n",
    "    Inputs and outputs are the same as regresser_loss_naive.\n",
    "    '''\n",
    "    _y = X.dot(W) # (404, 1)\n",
    "    loss = np.mean(np.square(y - _y)) + reg * 0.5 * np.sum(np.square(W))\n",
    "    grad = - 2 * X.T.dot(y - _y) / X.shape[0] + reg * W\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Check\n",
    "\n",
    "Performing a gradient check is as simple as comparing the analytic gradient to the numerical gradient. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, analytic_grad, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "        \n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x)\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x)\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad_numerical = (pos - neg) / (2 * h)\n",
    "        grad_analytic = analytic_grad[ix]\n",
    "        rel_error = abs(grad_numerical - grad_analytic) / (abs(grad_numerical) + abs(grad_analytic))\n",
    "        print('numerical: %f analytic: %f, relative error: %e' % (grad_numerical, grad_analytic, rel_error))\n",
    "        \n",
    "        it.iternext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient check with regularization turned off\n",
      "numerical: 1.450196 analytic: 1.450196, relative error: 2.500940e-09\n",
      "numerical: -3.240397 analytic: -3.240397, relative error: 1.674595e-09\n",
      "numerical: 4.489404 analytic: 4.489404, relative error: 4.655180e-09\n",
      "numerical: -1.516704 analytic: -1.516704, relative error: 1.586829e-08\n",
      "numerical: 3.886106 analytic: 3.886106, relative error: 3.878214e-09\n",
      "numerical: -3.181649 analytic: -3.181649, relative error: 1.135836e-09\n",
      "numerical: 4.368756 analytic: 4.368756, relative error: 3.308320e-09\n",
      "numerical: -1.940520 analytic: -1.940520, relative error: 2.548463e-09\n",
      "numerical: 5.329158 analytic: 5.329158, relative error: 2.154025e-09\n",
      "numerical: 5.378774 analytic: 5.378774, relative error: 4.449455e-09\n",
      "numerical: 4.349617 analytic: 4.349617, relative error: 1.702340e-09\n",
      "numerical: -2.730431 analytic: -2.730431, relative error: 7.580849e-10\n",
      "numerical: 5.300355 analytic: 5.300355, relative error: 1.074430e-09\n",
      "numerical: -45.044554 analytic: -45.044554, relative error: 6.107941e-10\n",
      "\n",
      "gradient check with regularization turned on\n",
      "numerical: 1.450196 analytic: 1.450196, relative error: 1.418766e-09\n",
      "numerical: -3.240397 analytic: -3.240397, relative error: 7.974884e-10\n",
      "numerical: 4.489404 analytic: 4.489404, relative error: 4.094933e-10\n",
      "numerical: -1.516704 analytic: -1.516704, relative error: 9.969246e-10\n",
      "numerical: 3.886106 analytic: 3.886106, relative error: 2.213770e-10\n",
      "numerical: -3.181649 analytic: -3.181649, relative error: 2.425350e-10\n",
      "numerical: 4.368756 analytic: 4.368756, relative error: 5.548183e-11\n",
      "numerical: -1.940520 analytic: -1.940520, relative error: 1.083819e-09\n",
      "numerical: 5.329158 analytic: 5.329158, relative error: 2.072658e-11\n",
      "numerical: 5.378774 analytic: 5.378774, relative error: 2.222151e-10\n",
      "numerical: 4.349617 analytic: 4.349617, relative error: 3.954798e-10\n",
      "numerical: -2.730431 analytic: -2.730431, relative error: 2.828389e-10\n",
      "numerical: 5.300355 analytic: 5.300355, relative error: 1.984443e-12\n",
      "numerical: -45.044554 analytic: -45.044554, relative error: 2.017475e-11\n"
     ]
    }
   ],
   "source": [
    "W = np.zeros((X_train.shape[1], 1))\n",
    "\n",
    "# do the gradient check with regularization turned off\n",
    "print('gradient check with regularization turned off')\n",
    "loss, grad = regresser_loss_naive(W, X_train, y_train, 0.0)\n",
    "f = lambda w: regresser_loss_naive(w, X_train, y_train, 0.0)[0]\n",
    "grad_numerical = eval_numerical_gradient(f, W, grad)\n",
    "\n",
    "# do the gradient check once again with regularization turned on\n",
    "print('\\ngradient check with regularization turned on')\n",
    "loss, grad = regresser_loss_vectorized(W, X_train, y_train, 0.5)\n",
    "f = lambda w: regresser_loss_vectorized(w, X_train, y_train, 0.5)[0]\n",
    "grad_numerical = eval_numerical_gradient(f, W, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Comparison\n",
    "\n",
    "compare the performance of the naive implementation and vetorized implementation of gradient computing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive implementation computed in 0.012332s\n",
      "Vectorized implementation computed in 0.000303s\n",
      "difference of loss: 0.000000\n",
      "difference of gradient: 0.000000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = regresser_loss_naive(W, X_train, y_train, 0.5)\n",
    "toc = time.time()\n",
    "print('Naive implementation computed in %fs' % (toc - tic))\n",
    "\n",
    "\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = regresser_loss_vectorized(W, X_train, y_train, 0.5)\n",
    "toc = time.time()\n",
    "print('Vectorized implementation computed in %fs' % (toc - tic))\n",
    "\n",
    "\n",
    "print('difference of loss: %f' % (loss_naive - loss_vectorized))\n",
    "difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('difference of gradient: %f' % difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 3000: train loss: 588.034356\tvaluation loss: 606.274156\n",
      "iteration 100 / 3000: train loss: 414.155465\tvaluation loss: 437.078733\n",
      "iteration 200 / 3000: train loss: 310.078673\tvaluation loss: 334.630586\n",
      "iteration 300 / 3000: train loss: 247.436201\tvaluation loss: 272.148823\n",
      "iteration 400 / 3000: train loss: 209.531183\tvaluation loss: 233.745890\n",
      "iteration 500 / 3000: train loss: 186.468383\tvaluation loss: 209.931917\n",
      "iteration 600 / 3000: train loss: 172.350288\tvaluation loss: 195.005927\n",
      "iteration 700 / 3000: train loss: 163.645497\tvaluation loss: 185.526136\n",
      "iteration 800 / 3000: train loss: 158.230977\tvaluation loss: 179.405345\n",
      "iteration 900 / 3000: train loss: 154.825866\tvaluation loss: 175.372308\n",
      "iteration 1000 / 3000: train loss: 152.654814\tvaluation loss: 172.649238\n",
      "iteration 1100 / 3000: train loss: 151.246875\tvaluation loss: 170.757855\n",
      "iteration 1200 / 3000: train loss: 150.314934\tvaluation loss: 169.402301\n",
      "iteration 1300 / 3000: train loss: 149.683177\tvaluation loss: 168.398240\n",
      "iteration 1400 / 3000: train loss: 149.243363\tvaluation loss: 167.629786\n",
      "iteration 1500 / 3000: train loss: 148.928400\tvaluation loss: 167.023271\n",
      "iteration 1600 / 3000: train loss: 148.696333\tvaluation loss: 166.531221\n",
      "iteration 1700 / 3000: train loss: 148.520636\tvaluation loss: 166.122521\n",
      "iteration 1800 / 3000: train loss: 148.384297\tvaluation loss: 165.776375\n",
      "iteration 1900 / 3000: train loss: 148.276216\tvaluation loss: 165.478559\n",
      "iteration 2000 / 3000: train loss: 148.189001\tvaluation loss: 165.219098\n",
      "iteration 2100 / 3000: train loss: 148.117605\tvaluation loss: 164.990800\n",
      "iteration 2200 / 3000: train loss: 148.058495\tvaluation loss: 164.788343\n",
      "iteration 2300 / 3000: train loss: 148.009126\tvaluation loss: 164.607673\n",
      "iteration 2400 / 3000: train loss: 147.967615\tvaluation loss: 164.445632\n",
      "iteration 2500 / 3000: train loss: 147.932532\tvaluation loss: 164.299698\n",
      "iteration 2600 / 3000: train loss: 147.902764\tvaluation loss: 164.167819\n",
      "iteration 2700 / 3000: train loss: 147.877429\tvaluation loss: 164.048297\n",
      "iteration 2800 / 3000: train loss: 147.855816\tvaluation loss: 163.939706\n",
      "iteration 2900 / 3000: train loss: 147.837344\tvaluation loss: 163.840837\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# reg: regularization strength.\n",
    "# lr: learning rate for optimization.\n",
    "# num_iters: number of steps to take when optimizing\n",
    "reg = 0.5\n",
    "lr = 0.001\n",
    "num_iters=3000\n",
    "\n",
    "\n",
    "W = np.zeros((X_train.shape[1], 1))\n",
    "loss_train_history = []\n",
    "loss_val_history = []\n",
    "for i in range(num_iters):\n",
    "    loss_train, grad = regresser_loss_vectorized(W, X_train, y_train, reg)\n",
    "    W -= lr * grad\n",
    "    \n",
    "    loss_val, _ = regresser_loss_vectorized(W, X_val, y_val, reg)\n",
    "    \n",
    "    loss_train_history.append(loss_train)\n",
    "    loss_val_history.append(loss_val)\n",
    "    if i % 100 == 0:\n",
    "        print('iteration %d / %d: train loss: %f\\tvaluation loss: %f' \n",
    "              % (i, num_iters, loss_train, loss_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA7YAAAOjCAYAAACP4ZiwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb898c54f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss as a function of iteration number:\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Train loss')\n",
    "plt.plot(loss_train_history)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Valuation loss')\n",
    "plt.plot(loss_val_history)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Use the validation set to tune hyperparameters (regularization strength and learning rate).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 1.063455e-06, reg 4.479741e+00, train loss: 576.476659, validation loss: 576.476659\n",
      "lr 1.124548e-06, reg 5.744144e+00, train loss: 575.871422, validation loss: 575.871422\n",
      "lr 1.224474e-06, reg 4.569962e-05, train loss: 574.591339, validation loss: 574.591339\n",
      "lr 1.323595e-06, reg 2.476017e-03, train loss: 573.518216, validation loss: 573.518216\n",
      "lr 1.390362e-06, reg 5.298693e-02, train loss: 572.799208, validation loss: 572.799208\n",
      "lr 1.888879e-06, reg 5.602958e+03, train loss: 587.637655, validation loss: 587.637655\n",
      "lr 1.954763e-06, reg 9.775005e-01, train loss: 566.836031, validation loss: 566.836031\n",
      "lr 2.119718e-06, reg 1.568044e+02, train loss: 576.696351, validation loss: 576.696351\n",
      "lr 2.216526e-06, reg 8.168044e-04, train loss: 563.949499, validation loss: 563.949499\n",
      "lr 2.452968e-06, reg 8.684048e-03, train loss: 561.446788, validation loss: 561.446788\n",
      "lr 3.238008e-06, reg 3.850527e+04, train loss: 587.976604, validation loss: 587.976604\n",
      "lr 3.257046e-06, reg 7.962230e+02, train loss: 585.252209, validation loss: 585.252209\n",
      "lr 3.274356e-06, reg 1.911301e-04, train loss: 552.842097, validation loss: 552.842097\n",
      "lr 3.396532e-06, reg 1.943474e-05, train loss: 551.574905, validation loss: 551.574905\n",
      "lr 3.555357e-06, reg 4.564973e+02, train loss: 583.197246, validation loss: 583.197246\n",
      "lr 3.612017e-06, reg 1.902316e-04, train loss: 549.347929, validation loss: 549.347929\n",
      "lr 3.686449e-06, reg 5.536343e+02, train loss: 584.040147, validation loss: 584.040147\n",
      "lr 3.702369e-06, reg 1.288427e-03, train loss: 548.417517, validation loss: 548.417517\n",
      "lr 3.912497e-06, reg 3.607843e-02, train loss: 546.273401, validation loss: 546.273401\n",
      "lr 4.532611e-06, reg 1.660191e-02, train loss: 539.954185, validation loss: 539.954185\n",
      "lr 4.633554e-06, reg 1.079829e-02, train loss: 538.931333, validation loss: 538.931333\n",
      "lr 4.649229e-06, reg 6.001800e+02, train loss: 584.348170, validation loss: 584.348170\n",
      "lr 5.304266e-06, reg 8.655894e-01, train loss: 532.814765, validation loss: 532.814765\n",
      "lr 5.542404e-06, reg 4.167976e-04, train loss: 529.837644, validation loss: 529.837644\n",
      "lr 5.641113e-06, reg 5.584656e-04, train loss: 528.861039, validation loss: 528.861039\n",
      "lr 5.961854e-06, reg 1.181034e+03, train loss: 586.156297, validation loss: 586.156297\n",
      "lr 6.145189e-06, reg 2.615144e+04, train loss: 587.949326, validation loss: 587.949326\n",
      "lr 9.203082e-06, reg 3.247533e+02, train loss: 581.251821, validation loss: 581.251821\n",
      "lr 1.070288e-05, reg 7.867163e-05, train loss: 481.369772, validation loss: 481.369772\n",
      "lr 1.074348e-05, reg 1.529163e-01, train loss: 481.414736, validation loss: 481.414736\n",
      "lr 1.092530e-05, reg 1.406971e+02, train loss: 572.579343, validation loss: 572.579343\n",
      "lr 1.121800e-05, reg 1.044866e+04, train loss: 587.821575, validation loss: 587.821575\n",
      "lr 1.147969e-05, reg 5.551067e-02, train loss: 474.679671, validation loss: 474.679671\n",
      "lr 1.155713e-05, reg 3.402745e+00, train loss: 483.592271, validation loss: 483.592271\n",
      "lr 1.159402e-05, reg 6.125337e-04, train loss: 473.515227, validation loss: 473.515227\n",
      "lr 1.206320e-05, reg 3.039979e+02, train loss: 580.793480, validation loss: 580.793480\n",
      "lr 1.360868e-05, reg 2.675235e+01, train loss: 523.813165, validation loss: 523.813165\n",
      "lr 1.612081e-05, reg 7.383036e-05, train loss: 435.770505, validation loss: 435.770505\n",
      "lr 1.675761e-05, reg 8.940719e+04, train loss: 588.009483, validation loss: 588.009483\n",
      "lr 1.683020e-05, reg 2.810683e+00, train loss: 445.654789, validation loss: 445.654789\n",
      "lr 2.206449e-05, reg 3.862671e+00, train loss: 423.235535, validation loss: 423.235535\n",
      "lr 2.259423e-05, reg 7.840991e-04, train loss: 387.582937, validation loss: 387.582937\n",
      "lr 2.267733e-05, reg 6.127353e-01, train loss: 392.814954, validation loss: 392.814954\n",
      "lr 2.459621e-05, reg 7.061902e-03, train loss: 374.003279, validation loss: 374.003279\n",
      "lr 2.537452e-05, reg 3.157334e-02, train loss: 369.133351, validation loss: 369.133351\n",
      "lr 2.625771e-05, reg 1.964291e+03, train loss: 586.903974, validation loss: 586.903974\n",
      "lr 2.884775e-05, reg 2.004814e+04, train loss: 587.923444, validation loss: 587.923444\n",
      "lr 2.929820e-05, reg 1.270487e-02, train loss: 344.172743, validation loss: 344.172743\n",
      "lr 2.939395e-05, reg 1.285469e-03, train loss: 343.428128, validation loss: 343.428128\n",
      "lr 3.907114e-05, reg 9.773090e-02, train loss: 292.381600, validation loss: 292.381600\n",
      "lr 4.913463e-05, reg 1.428937e+02, train loss: 572.805637, validation loss: 572.805637\n",
      "lr 5.002934e-05, reg 3.146993e-04, train loss: 241.468410, validation loss: 241.468410\n",
      "lr 5.567234e-05, reg 8.893522e-05, train loss: 220.246773, validation loss: 220.246773\n",
      "lr 6.180245e-05, reg 3.564604e+03, train loss: 587.411008, validation loss: 587.411008\n",
      "lr 6.340098e-05, reg 6.349901e+01, train loss: 554.665974, validation loss: 554.665974\n",
      "lr 6.663619e-05, reg 2.931367e-02, train loss: 186.484732, validation loss: 186.484732\n",
      "lr 6.717798e-05, reg 3.371430e-05, train loss: 183.778257, validation loss: 183.778257\n",
      "lr 6.807257e-05, reg 7.044528e+00, train loss: 380.374915, validation loss: 380.374915\n",
      "lr 7.162092e-05, reg 3.096622e-01, train loss: 184.873304, validation loss: 184.873304\n",
      "lr 8.080985e-05, reg 3.959034e-02, train loss: 151.983759, validation loss: 151.983759\n",
      "lr 8.192886e-05, reg 4.069907e-05, train loss: 147.757664, validation loss: 147.757664\n",
      "lr 8.547564e-05, reg 8.135014e-03, train loss: 140.950628, validation loss: 140.950628\n",
      "lr 8.758227e-05, reg 2.197953e+00, train loss: 237.123472, validation loss: 237.123472\n",
      "lr 9.819904e-05, reg 1.611581e-04, train loss: 118.505120, validation loss: 118.505120\n",
      "lr 1.004832e-04, reg 8.356741e+03, train loss: 587.768330, validation loss: 587.768330\n",
      "lr 1.036857e-04, reg 7.378721e-04, train loss: 110.592951, validation loss: 110.592951\n",
      "lr 1.099880e-04, reg 8.156935e+04, train loss: 703.816073, validation loss: 703.816073\n",
      "lr 1.229230e-04, reg 8.287683e+03, train loss: 587.766114, validation loss: 587.766114\n",
      "lr 1.315128e-04, reg 2.851848e-05, train loss: 81.014305, validation loss: 81.014305\n",
      "lr 1.338787e-04, reg 3.908865e-04, train loss: 79.173463, validation loss: 79.173463\n",
      "lr 1.388977e-04, reg 1.367250e+02, train loss: 572.134015, validation loss: 572.134015\n",
      "lr 1.529871e-04, reg 1.502426e+04, train loss: 615.870800, validation loss: 615.870800\n",
      "lr 1.659275e-04, reg 1.750098e-05, train loss: 60.583565, validation loss: 60.583565\n",
      "lr 1.939222e-04, reg 8.945714e+04, train loss: 3846.344745, validation loss: 3846.344745\n",
      "lr 2.164897e-04, reg 9.972192e+04, train loss: 411.185758, validation loss: 411.185758\n",
      "lr 2.248682e-04, reg 8.496296e-01, train loss: 98.953698, validation loss: 98.953698\n",
      "lr 2.422915e-04, reg 2.733500e+01, train loss: 515.117965, validation loss: 515.117965\n",
      "lr 2.675529e-04, reg 3.163939e+04, train loss: 828.726339, validation loss: 828.726339\n",
      "lr 2.874548e-04, reg 4.075901e+03, train loss: 587.489143, validation loss: 587.489143\n",
      "lr 2.992956e-04, reg 4.391327e+03, train loss: 587.528277, validation loss: 587.528277\n",
      "lr 3.096974e-04, reg 1.737117e-01, train loss: 43.868215, validation loss: 43.868215\n",
      "lr 3.214506e-04, reg 1.344144e-05, train loss: 36.276003, validation loss: 36.276003\n",
      "lr 3.503615e-04, reg 2.152756e-01, train loss: 43.731054, validation loss: 43.731054\n",
      "lr 3.553459e-04, reg 7.133722e+00, train loss: 373.582073, validation loss: 373.582073\n",
      "lr 3.671458e-04, reg 1.106631e+02, train loss: 568.493020, validation loss: 568.493020\n",
      "lr 4.174777e-04, reg 4.709074e-03, train loss: 32.956147, validation loss: 32.956147\n",
      "lr 4.335075e-04, reg 3.823579e-03, train loss: 32.520498, validation loss: 32.520498\n",
      "lr 4.340542e-04, reg 2.066824e+02, train loss: 577.435221, validation loss: 577.435221\n",
      "lr 4.427221e-04, reg 7.450655e+02, train loss: 585.061976, validation loss: 585.061976\n",
      "lr 4.429781e-04, reg 2.929122e+04, train loss: 3296.816153, validation loss: 3296.816153\n",
      "lr 5.532925e-04, reg 2.612979e-01, train loss: 41.287227, validation loss: 41.287227\n",
      "lr 6.223564e-04, reg 2.052386e-05, train loss: 28.952580, validation loss: 28.952580\n",
      "lr 6.839791e-04, reg 2.203615e-04, train loss: 28.168093, validation loss: 28.168093\n",
      "lr 7.212483e-04, reg 3.417750e-04, train loss: 27.750417, validation loss: 27.750417\n",
      "lr 7.310619e-04, reg 9.377806e-04, train loss: 27.658193, validation loss: 27.658193\n",
      "lr 7.638578e-04, reg 7.124676e+04, train loss: 6480.251655, validation loss: 6480.251655\n",
      "lr 7.756818e-04, reg 4.630186e+00, train loss: 306.973052, validation loss: 306.973052\n",
      "lr 7.932213e-04, reg 2.032194e+04, train loss: 276.925442, validation loss: 276.925442\n",
      "lr 8.976080e-04, reg 2.709654e+01, train loss: 514.543969, validation loss: 514.543969\n",
      "lr 9.568677e-04, reg 4.181579e-01, train loss: 50.264221, validation loss: 50.264221\n",
      "lowest validation loss achieved: 27.658193\n"
     ]
    }
   ],
   "source": [
    "# first run coarse search\n",
    "# If the cost is 3 times greater than origin cost,break out early to avoid explosions\n",
    "\n",
    "max_count = 100\n",
    "\n",
    "results = {}\n",
    "lowest_loss = float('inf')   # The lowest validation loss that we have seen so far.\n",
    "best_W = None # The weight matrix that achieved thelowest validation loss.\n",
    "best_setup = None # the best hyperparameter to get the best_W, (lr, reg, num_iters)\n",
    "\n",
    "for i in range(max_count):\n",
    "    W = np.zeros((X_train.shape[1], 1))\n",
    "    reg = 10**np.random.uniform(-5, 5)\n",
    "    lr = 10**np.random.uniform(-6, -3)\n",
    "    \n",
    "    loss_init, _ = regresser_loss_vectorized(W, X_train, y_train, reg)\n",
    "    for i in range(5000):\n",
    "        loss, grad = regresser_loss_vectorized(W, X_train, y_train, reg)\n",
    "        W -= lr * grad\n",
    "        if loss > 3 * loss_init:\n",
    "            break\n",
    "    loss_train, _ = regresser_loss_vectorized(W, X_train, y_train, 0)\n",
    "    loss_val, _ = regresser_loss_vectorized(W, X_train, y_train, 0)\n",
    "    \n",
    "    results[lr, reg] = loss_train, loss_val\n",
    "    if loss_val < lowest_loss:\n",
    "        lowest_loss = loss_val\n",
    "        best_W = W\n",
    "        best_setup = (lr, reg, num_iters)\n",
    "\n",
    "\n",
    "for lr, reg in sorted(results):\n",
    "    loss_train, loss_val = results[(lr, reg)]\n",
    "    print('lr %e, reg %e, train loss: %f, validation loss: %f' % (\n",
    "                lr, reg, loss_train, loss_val))\n",
    "    \n",
    "print('lowest validation loss achieved: %f' % lowest_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-df279c34132b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-df279c34132b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    lr 1.256167e-04, reg 5.228679e-04, train loss: 98.342621, validation loss: 98.342621\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "lr 1.256167e-04, reg 5.228679e-04, train loss: 98.342621, validation loss: 98.342621\n",
    "lr 1.265925e-04, reg 1.734536e-04, train loss: 97.582745, validation loss: 97.582745\n",
    "lr 1.577575e-04, reg 2.114790e-05, train loss: 77.183114, validation loss: 77.183114\n",
    "lr 1.822775e-04, reg 2.886181e-03, train loss: 65.172677, validation loss: 65.172677\n",
    "lr 1.999056e-04, reg 3.734254e-05, train loss: 57.934372, validation loss: 57.934372\n",
    "lr 2.364170e-04, reg 1.358089e-01, train loss: 54.239104, validation loss: 54.239104\n",
    "lr 3.274482e-04, reg 1.557479e-04, train loss: 30.778259, validation loss: 30.778259\n",
    "lr 3.354292e-04, reg 7.033270e-03, train loss: 30.287093, validation loss: 30.287093\n",
    "lr 4.357737e-04, reg 2.642519e-05, train loss: 23.004004, validation loss: 23.004004\n",
    "lr 6.226358e-04, reg 3.813791e-05, train loss: 18.399611, validation loss: 18.399611\n",
    "lr 7.604731e-04, reg 1.564428e-01, train loss: 24.099079, validation loss: 24.099079\n",
    "lr 8.707518e-04, reg 6.222566e-05, train loss: 16.199228, validation loss: 16.199228\n",
    "lr 9.809586e-04, reg 2.611612e-02, train loss: 16.224287, validation loss: 16.224287"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_count = 100\n",
    "\n",
    "results = {}\n",
    "lowest_loss = float('inf')   # The lowest validation loss that we have seen so far.\n",
    "best_W = None # The weight matrix that achieved thelowest validation loss.\n",
    "best_setup = None # the best hyperparameter to get the best_W, (lr, reg, num_iters)\n",
    "\n",
    "for i in range(max_count):\n",
    "    W = np.zeros((X_train.shape[1], 1))\n",
    "    reg = 10**np.random.uniform(-5, 0)\n",
    "    lr = 10**np.random.uniform(-4, -1)\n",
    "    \n",
    "    loss_init, _ = regresser_loss_vectorized(W, X_train, y_train, reg)\n",
    "    for i in range(3000):\n",
    "        loss, grad = regresser_loss_vectorized(W, X_train, y_train, reg)\n",
    "        W -= lr * grad\n",
    "        if loss > 3 * loss_init:\n",
    "            break\n",
    "    loss_train, _ = regresser_loss_vectorized(W, X_train, y_train, 0)\n",
    "    loss_val, _ = regresser_loss_vectorized(W, X_train, y_train, 0)\n",
    "    \n",
    "    results[lr, reg] = loss_train, loss_val\n",
    "    if loss_val < lowest_loss:\n",
    "        lowest_loss = loss_val\n",
    "        best_W = W\n",
    "        best_setup = (lr, reg, num_iters)\n",
    "\n",
    "\n",
    "for lr, reg in sorted(results):\n",
    "    loss_train, loss_val = results[(lr, reg)]\n",
    "    print('lr %e, reg %e, train loss: %f, validation loss: %f' % (\n",
    "                lr, reg, loss_train, loss_val))\n",
    "    \n",
    "print('lowest validation loss achieved: %f' % lowest_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr, reg in results:\n",
    "    loss_train, loss_val = results[(lr, reg)]\n",
    "    print('lr %e, reg %e, train loss: %f, validation loss: %f' % (\n",
    "                lr, reg, loss_train, loss_val))\n",
    "    \n",
    "print('lowest validation loss achieved: %f' % lowest_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(np.square(X_train.dot(best_W) - y_train)))\n",
    "print(np.mean(np.square(X_val.dot(best_W) - y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss as a function of iteration number:\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Train loss')\n",
    "plt.plot(loss_train_history)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Valuation loss')\n",
    "plt.plot(loss_val_history)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
